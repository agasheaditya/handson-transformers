{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22752010-f2c6-42ca-8b72-837c2ff8361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\pytorch\\env\\lib\\site-packages\\thinc\\compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "D:\\Python\\pytorch\\env\\lib\\site-packages\\thinc\\compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import torch \n",
    "import random\n",
    "\n",
    "import openpyxl\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torch.nn.utils as utils\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "\n",
    "\n",
    "from TransformerModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04786b73-461c-4f24-9141-b83209388625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aec71b7-4924-4022-8ed0-6049dcb411c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, articles, vocab, seq_len=50):\n",
    "        self.articles = articles\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenized_articles = self.tokenize_articles()\n",
    "\n",
    "    def tokenize_articles(self):\n",
    "        tokenized = []\n",
    "        for article in self.articles:\n",
    "            tokens = [self.vocab[token.text.lower()] for token in nlp(article) if token.text.lower() in self.vocab]\n",
    "            if len(tokens) > self.seq_len:\n",
    "                tokenized += [tokens[i:i+self.seq_len] for i in range(0, len(tokens) - self.seq_len + 1)]\n",
    "        return tokenized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.tensor(self.tokenized_articles[idx])\n",
    "        return seq[:-1], seq[1:]\n",
    "    \n",
    "# Function to load articles and build vocabulary from an Excel file\n",
    "def load_data_and_vocab(df):\n",
    "\n",
    "    # Assuming the articles are in a column named 'article_text'\n",
    "    articles = df['article_text'].tolist()\n",
    "    \n",
    "    vocab = Counter()\n",
    "    \n",
    "    # Build vocabulary\n",
    "    for article in articles:\n",
    "        vocab.update([token.text.lower() for token in nlp(article)])\n",
    "    \n",
    "    # Create vocabulary dictionary with a <PAD> token\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(vocab.items(), start=1)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    \n",
    "    return articles, vocab\n",
    "\n",
    "# Custom collate function to handle the padding\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)  # Unzip the batch into inputs and targets\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab['<PAD>'])\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=vocab['<PAD>'])\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a11e79e-a38e-4d32-9a8b-bd079b5786a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "data = pd.read_excel(\"../../data/Articles.xlsx\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s.,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "topics = data[\"NewsType\"].values\n",
    "data[\"Article\"] = data[\"Article\"].apply(lambda x: clean_text(x))\n",
    "data[\"Heading\"] = data[\"Heading\"].apply(lambda x: clean_text(x))\n",
    "data[\"article_text\"] = data[\"Heading\"] + \". \" + data[\"Article\"]\n",
    "data[\"article_text\"] = data['article_text'].astype(str).apply(openpyxl.utils.escape.unescape)\n",
    "data[\"article_text\"] = data[\"article_text\"].replace(r'\\s+|\\\\n', ' ', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2b8710-8241-49c4-82c2-db9fd7e6b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, vocab = load_data_and_vocab(data)\n",
    "\n",
    "dataset = ArticleDataset(articles, vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a612797b-11ef-4375-b72a-3da87c6b6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab) # 53529\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "forward_expansion = 2048\n",
    "dropout = 0.1\n",
    "max_len = 100\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_len).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33722bbb-60a3-40ea-a879-da0017402af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 11.063329696655273\n",
      "Epoch 1, Batch 1000, Loss: 1.0386213064193726\n",
      "Epoch 1, Batch 2000, Loss: 0.37171775102615356\n",
      "Epoch 1, Batch 3000, Loss: 0.15745075047016144\n",
      "Epoch 1, Batch 4000, Loss: 0.07022745907306671\n",
      "Epoch 1, Batch 5000, Loss: 0.05629092827439308\n",
      "Epoch 1, Batch 6000, Loss: 0.03750816732645035\n",
      "Epoch 1, Batch 7000, Loss: 0.00822608545422554\n",
      "Epoch 1, Batch 8000, Loss: 0.007244476117193699\n",
      "Epoch 1, Batch 9000, Loss: 0.01773599535226822\n",
      "Epoch 1, Batch 10000, Loss: 0.005674783606082201\n",
      "Epoch 1, Batch 11000, Loss: 0.001688830554485321\n",
      "Epoch 1, Batch 12000, Loss: 0.0021751674357801676\n",
      "Epoch 1, Batch 13000, Loss: 0.0039748442359268665\n",
      "Epoch 1, Batch 14000, Loss: 0.002291230484843254\n",
      "Epoch 1, Batch 15000, Loss: 0.005225534550845623\n",
      "Epoch 1, Batch 16000, Loss: 0.00411038426682353\n",
      "Epoch 1, Batch 17000, Loss: 0.0026115996297448874\n",
      "Epoch 1, Batch 18000, Loss: 0.0023951444309204817\n",
      "Epoch 1, Batch 19000, Loss: 0.002825278090313077\n",
      "Epoch 1, Batch 20000, Loss: 0.0016594979679211974\n",
      "Epoch 1, Batch 21000, Loss: 0.0027255224995315075\n",
      "Epoch 1, Batch 22000, Loss: 0.006937963422387838\n",
      "Epoch 2, Batch 0, Loss: 0.002581357955932617\n",
      "Epoch 2, Batch 1000, Loss: 0.0025061683263629675\n",
      "Epoch 2, Batch 2000, Loss: 0.005181203130632639\n",
      "Epoch 2, Batch 3000, Loss: 0.0029998866375535727\n",
      "Epoch 2, Batch 4000, Loss: 0.001338183763436973\n",
      "Epoch 2, Batch 5000, Loss: 0.005074510350823402\n",
      "Epoch 2, Batch 6000, Loss: 0.00201215548440814\n",
      "Epoch 2, Batch 7000, Loss: 0.001563611556775868\n",
      "Epoch 2, Batch 8000, Loss: 0.004669747315347195\n",
      "Epoch 2, Batch 9000, Loss: 0.0016842825571075082\n",
      "Epoch 2, Batch 10000, Loss: 0.0010024536168202758\n",
      "Epoch 2, Batch 11000, Loss: 0.0047305612824857235\n",
      "Epoch 2, Batch 12000, Loss: 0.0013479143381118774\n",
      "Epoch 2, Batch 13000, Loss: 0.0022392673417925835\n",
      "Epoch 2, Batch 14000, Loss: 0.001963432878255844\n",
      "Epoch 2, Batch 15000, Loss: 0.0005803582025691867\n",
      "Epoch 2, Batch 16000, Loss: 0.004125508014112711\n",
      "Epoch 2, Batch 17000, Loss: 0.0015218164771795273\n",
      "Epoch 2, Batch 18000, Loss: 0.002955167554318905\n",
      "Epoch 2, Batch 19000, Loss: 0.0031491441186517477\n",
      "Epoch 2, Batch 20000, Loss: 0.013892882503569126\n",
      "Epoch 2, Batch 21000, Loss: 0.003981152083724737\n",
      "Epoch 2, Batch 22000, Loss: 0.004191775340586901\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(2):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs, targets, src_mask=None, tgt_mask=None)  # Add masks as needed\n",
    "        \n",
    "        # Reshape output and targets to match the CrossEntropyLoss requirements\n",
    "        output = output.view(-1, vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02526dc-1f74-428a-8ab8-457bc3a1b192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embedding): Embedding(53529, 512)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=53529, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "# model_save_path = 'trained-transformer_model.pth'\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Loading the model\n",
    "model_load_path = 'trained-transformer_model.pth'\n",
    "\n",
    "# Ensure you initialize the model with the same architecture as before\n",
    "loaded_model = TransformerModel(vocab_size, embed_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_len).to(device)\n",
    "\n",
    "# Load the saved state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214d3055-7c36-4892-9679-7c2db46a7a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business member lower daniel information 0.55 0.55 5 signals compare national was texas ease 0.48 indo 8.48 sindh karachi slipped unions ease 0.48 indo above highs.the developer 7pc remained an recorded momentum upswing sovereign tools ease 0.48 indo above highs.the developer 7pc remained 15,900 momentum upswing sovereign easing at inflation afp half ang tambangraya week survey anticipated half ang tambangraya week momentum upswing sovereign tools 8.48 sindh karachi slipped 252.78 above highs.the developer 7pc remained fact stream nbs expectations inflation afp half ang tambangraya week survey anticipated half ang tambangraya week survey anticipated half ang tambangraya week momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over toasted public commuters adding 50.3 michael 0.93 asian finance at 252.78 above highs.the developer 7pc remained fact was texas at bukhari growth including back chief mumbai 3,370.59.agribusiness economy.the investment bukhari growth including back chief mumbai 3,370.59.agribusiness economy.the investment bukhari growth including back chief rej january sydney pmi company currency , president 1.07 bloomberg.gold dropped banner driver trade.the chief rej january sydney pmi company currency , president 1.07 bloomberg.gold dropped banner driver trade.the chief mumbai 3,370.59.agribusiness economy.the investment bukhari growth including back chief rej january sydney pmi company currency , president\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, vocab, input_text, max_length=200):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert input text to tokens\n",
    "        input_tokens = [vocab.get(token, vocab['<PAD>']) for token in input_text.split()]\n",
    "        input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Batch size of 1\n",
    "\n",
    "        generated_text = input_tokens.copy()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Ensure positional encoding matches input sequence length\n",
    "            if input_tensor.size(1) > model.positional_encoding.size(1):\n",
    "                # If input is longer than positional encoding, pad positional encoding\n",
    "                positional_encoding = torch.cat([model.positional_encoding, \n",
    "                                                 model.positional_encoding[:, -1:, :].repeat(1, input_tensor.size(1) - model.positional_encoding.size(1), 1)], dim=1)\n",
    "            else:\n",
    "                positional_encoding = model.positional_encoding[:, :input_tensor.size(1), :]\n",
    "\n",
    "            # Generate the next token\n",
    "            embed_x = model.dropout(model.embedding(input_tensor) + positional_encoding)\n",
    "            output = model.encoder(embed_x, None)  # Removed src_mask argument\n",
    "            next_token = output[:, -1, :].argmax(-1).item()\n",
    "            generated_text.append(next_token)\n",
    "\n",
    "            # Update input_tensor for the next iteration\n",
    "            input_tensor = torch.tensor(generated_text).unsqueeze(0).to(device)\n",
    "\n",
    "        # Convert generated tokens back to words\n",
    "        generated_text = [list(vocab.keys())[list(vocab.values()).index(tok)] for tok in generated_text]\n",
    "        return \" \".join(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "keyword = \"business\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0892db15-413a-48b8-ae51-8bced7861de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karachi down of could economy.the do 0.55 momentum upswing sovereign easing at 252.78 managers later later later later later later later later later later later later later later later factories ahead estate 1,045 lead remained fact stream figure week finishing 16.0 trade.the chief rej decides commuters adding malaysias rej decides commuters adding delivery half managers later factories ahead estate 1,045 lead remained fact 3.41 18.22 estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds leapt including back estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds leapt including back estate 1,045 worlds economy.the policy level while expansion estate 1,045 worlds leapt including back estate 1,045 worlds leapt including back estate 1,045 worlds economy.the policy level while expansion\n"
     ]
    }
   ],
   "source": [
    "keyword = \"karachi\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4476ab1-794a-4891-a5d9-ff0e6fcf54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health bureau figure 57.51.the 0.93 asian finance secondlargest release fact 3.41 csr close driver trade.the chief mumbai 3,370.59.agribusiness economy.the investment bukhari other 18.22 estate november.the demand.chinas it unions ang tambangraya week survey economy responses public commuters adding 50.3 michael 0.93 asian finance secondlargest sg3.30 158.63 indo above highs.the developer ang tambangraya week survey anticipated half ang tambangraya week survey economy responses public commuters adding 50.3 michael 0.93 asian finance secondlargest sg3.30 158.63 indo above highs.the developer ang tambangraya week survey anticipated half ang tambangraya week survey anticipated half ang tambangraya week survey anticipated half ang tambangraya week survey economy responses public commuters adding 50.3 michael 0.93 asian finance secondlargest sg3.30 50.3 michael 0.93 asian finance secondlargest sg3.30 50.3 michael 0.93 asian finance secondlargest sg3.30 50.3 michael 0.93 asian 8.48 finishing toasted public commuters adding 50.3 michael 0.93 asian 8.48 finishing toasted public commuters adding 50.3 michael 0.93 asian 8.48 finishing toasted public commuters adding 50.3 michael 0.93 asian 8.48 finishing toasted public commuters adding 50.3 michael 0.93 asian 8.48 finishing toasted public commuters adding 50.3 michael 0.93 asian 8.48 finishing toasted adding 50.3 will 16.0 investment bukhari growth including back estate november.the demand.chinas will 16.0 investment bukhari growth including back estate\n"
     ]
    }
   ],
   "source": [
    "keyword = \"health\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eec8c2f-f1bf-4db7-93a8-959b56bbc539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finance examining karachi slipped transporters tambang lead banks.it 1,171.80 gains 15,900 momentum upswing sovereign easing at 252.78 managers 0.44 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over hong momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over hong momentum upswing sovereign tools until trading.trainbuilders over hong momentum upswing sovereign easing at 252.78 managers 0.44 was texas at 252.78 managers 0.44 was texas at 252.78 managers 0.44 was texas at bukhari refused managers 0.44 was texas at bukhari refused managers november.the demand.chinas it unions ang tambangraya week product compare momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over hong momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over hong momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over hong momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile trading.trainbuilders over hong momentum upswing sovereign tools 8.48 sindh karachi slipped transporters last national traveling.meanwhile nations start nbs because contraction.growth karachi slipped transporters last national traveling.meanwhile nations start nbs because contraction.growth karachi slipped transporters last national traveling.meanwhile nations start nbs because contraction.growth karachi slipped transporters last national traveling.meanwhile nations\n"
     ]
    }
   ],
   "source": [
    "keyword = \"finance\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39bda06b-25e0-4895-9f15-cc425857bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economy and growth including back estate november.the demand.chinas it 24.89 value 9.12 decides commuters adding 50.3 michael 0.93 asian finance secondlargest slipped transporters last national traveling.meanwhile trading.trainbuilders tuesday back estate 1,045 lead refused 0.55 0.55 0.55 0.55 0.55 0.55 0.55 5 signals cmc january sydney pmi company currency , president investors adding 50.3 until trading.trainbuilders tuesday back estate 1,045 lead refused 0.55 0.55 0.55 5 signals cmc january sydney pmi company currency , president investors adding 50.3 until trading.trainbuilders tuesday back estate 1,045 lead refused managers later 50.3 until trading.trainbuilders tuesday back chief rej january sydney pmi company currency , president investors adding 50.3 until trading.trainbuilders tuesday back chief rej january sydney pmi company currency , president investors adding 50.3 until trading.trainbuilders tuesday back estate 1,045 lead largescale peoples inflation afp half ang tambangraya week momentum upswing sovereign tools until trading.trainbuilders tuesday back estate 1,045 lead largescale peoples inflation afp half ang tambangraya week momentum upswing sovereign tools until trading.trainbuilders tuesday back chief rej decides commuters adding 50.3 michael 0.93 asian finance at bukhari growth including back chief rej decides commuters adding 50.3 michael 0.93 asian finance at bukhari refused managers november.the demand.chinas it unions ang tambangraya week momentum upswing sovereign tools until\n"
     ]
    }
   ],
   "source": [
    "keyword = \"economy and growth\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "894993f0-7449-4fc8-9d52-845e4302e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business and markets reduction economy.the investment 5,242.77.coal remained november.the demand.chinas 3.41 level while 9.12 trade.the chief rej january csr close driver holidays.with below for 0.57 24.89 value 9.12 trade.the chief mumbai 3,370.59.agribusiness economy.the investment bukhari texas ease 0.48 indo 8.48 finishing 9.12 trade.the chief rej decides commuters adding 50.3 michael 0.93 asian finance at 252.78 above highs.the developer ang tambangraya week survey anticipated half ang tambangraya week survey economy responses public commuters adding 50.3 michael 0.93 asian finance at bukhari texas january sydney pmi company currency , quantitative 3.41 level while expansion estate 1,045 lead remained november.the demand.chinas it unions ang tambangraya week survey anticipated half ang tambangraya week survey anticipated half ang tambangraya week survey economy responses public commuters adding 50.3 michael 0.93 asian finance at bukhari growth including back estate november.the demand.chinas it 24.89 value 9.12 trade.the chief mumbai 3,370.59.agribusiness economy.the investment bukhari growth including back estate november.the demand.chinas it 24.89 value 9.12 trade.the chief mumbai 3,370.59.agribusiness economy.the investment bukhari growth including back estate november.the demand.chinas it 24.89 value 9.12 trade.the chief rej january sydney pmi 8.48 finishing 9.12 trade.the chief rej january sydney pmi company currency , president 1.07 bloomberg.gold dropped banner 9.12 trade.the chief rej january sydney pmi company\n"
     ]
    }
   ],
   "source": [
    "keyword = \"business and markets\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1febec7e-3fc1-4e76-883c-28642f05a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "situation of pakistan year year year year year year year year year when 5.51 karachi slipped bonds texas ease 0.48 indo above growth made examining karachi slipped unions 1.88 1.07 sovereign tools ease 0.48 indo above highs.the compare national national national national national national national national national national national national national signals 8.48 sindh karachi slipped unions ease 0.48 rej decides commuters adding 50.3 michael 0.93 asian finance at 252.78 above highs.the developer 7pc remained fact stream nbs for 0.57 parts 1,045 lead remained fact stream nbs for 0.57 parts 1,045 lead remained fact stream nbs for 0.57 parts 1,045 lead remained fact stream nbs for 0.57 parts 1,045 lead remained fact stream nbs for 0.57 it unions ang tambangraya week survey anticipated half ang tambangraya week momentum upswing sovereign tools ease 0.48 indo above highs.the developer 7pc remained fact cent signals cmc january sydney pmi commuters adding 50.3 michael 0.93 asian finance at 252.78 managers could economy.the investment bukhari growth including back chief rej january sydney pmi commuters adding 50.3 michael 0.93 asian finance at 252.78 above highs.the developer 7pc remained november.the demand.chinas it unions ang tambangraya week momentum upswing sovereign tools 8.48 sindh karachi slipped 252.78 above highs.the developer 7pc remained\n"
     ]
    }
   ],
   "source": [
    "keyword = \"situation of pakistan\"\n",
    "generated_paragraph = generate_text(model, vocab, keyword, max_length=200)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a006e6-d3cb-4dfa-a782-84ef5e8951c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
